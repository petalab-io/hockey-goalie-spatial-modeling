# hockey-goalie-spatial-modeling
Learning repository for ice hockey goaltender evaluation using PyMC and Bayesian spatial modeling.

---

# アイスホッケーのゴールキーパー評価におけるベイズ空間モデリング：PyMCによる実践的アプローチ

## 1. エグゼクティブサマリー：DXコンサルタントのための視点
デジタルトランスフォーメーション（DX）の最前線において、データ分析は単なる「集計」から「文脈の理解」へと進化しています。スポーツアナリティクス、特にアイスホッケーのゴールキーパー評価という具体的な事例は、この進化を体感するための絶好のサンドボックスです。

従来の評価指標であるセーブ率（Save Percentage, Sv%）は、ビジネスにおけるKPI（重要業績評価指標）の平均値と同様の陥りやすい罠を孕んでいます。それは、「すべてのシュート（またはすべての顧客接点）は等価である」という誤った仮定に基づいている点です。実際には、ブルーライン（ゴールから遠い位置）からの緩やかなシュートと、スロット（ゴール正面の至近距離）からのワンタイマーシュートでは、阻止できる確率は劇的に異なります。

本レポートは、40代のDX推進コンサルタントであり、プログラミング経験豊富な実務家を対象に、ベイズ統計モデリングを用いた空間解析の手法を体系的に解説するものです。具体的には、Pythonの確率的プログラミングライブラリであるPyMCを活用し、ゴールキーパーのセーブ確率を「氷上の位置関数」としてモデル化します。

ここでは、単にコードを写経するだけでなく、その背後にある**ガウス過程（Gaussian Process）やヒルベルト空間近似（HSGP）**といった高度な数学的概念を、中学数学レベルの直感（距離、関数、確率）に落とし込んで理解することを
目指します。これにより、統計検定2級レベルの知識を実務に応用可能な「空間モデリング能力」へと昇華させます。

## 2. 統計的アプローチの動機：なぜ「空間」が必要なのか
### 2.1 平均値の嘘とコンテキストの喪失
ビジネスデータ分析において、ABテストの結果を単なる「コンバージョン率の差」として見ることが危険であるように、ゴールキーパーの能力を「通算セーブ率」だけで語ることは、重要なシグナルを見落とす原因となります。

例えば、ゴールキーパーAとBがいたとします。

- ゴールキーパーA: 遠距離からの簡単なシュートを100本受け、95本止めた（Sv% = 95.0%）。
- ゴールキーパーB: 至近距離からの決定的なシュートを100本受け、90本止めた（Sv% = 90.0%）。
- 
数字上はAが優秀に見えますが、コンテキスト（シュートの難易度）を考慮すれば、Bの方がチームへの貢献度は高い可能性があります。この「難易度」を決定する最大の要因が**空間的配置（Spatial Location）**です。

## 2.2 空間相関（Spatial Correlation）の概念
私たちが構築するモデルの根幹には、「空間相関」という考え方があります。これは、「近くにあるものは、遠くにあるものよりも似ている」という地理学の第一法則に基づいています。

#### 中学数学レベルでの解説：
数直線を想像してください。$x=1$ の地点での気温が20度だったとします。$x=1.1$ の地点の気温はどうでしょうか？おそらく20度に近い値でしょう。しかし、$x=100$ の地点の気温は全く違うかもしれません。

この「距離が近ければ値も近くなるはずだ」という直感を数式で表現したものが空間相関です。ゴールキーパーの場合、「ある地点 $(x, y)$ でシュートを止めるのが得意なら、そのすぐ隣の地点 $(x+\delta, y+\delta)$ でも得意なはずだ」という仮定を置くことができます。これにより、データが存在しない地点の能力も、周囲のデータから「滑らかに」推測することが可能になります。

## 3. 数学的基礎：中学数学で理解するモデルの構造
PyMCでの実装に入る前に、モデルを構成する数学的な部品を分解し、その意味を直感的に理解します。

### 3.1 ベルヌーイ試行と確率関数
シュートの結果は二値です。「セーブ（成功）」か「ゴール（失敗）」か。これを統計学ではベルヌーイ試行と呼びます。

コイン投げを考えましょう。表が出る確率を $p$ とします。

通常、この $p$ は定数（例：$0.5$）ですが、今回のモデルでは $p$ は定数ではなく、氷上の位置 $(x, y)$ に依存して変化する関数として扱います。

$$p_{save} = f(x, y)$$

ここで、
- $x$: ゴールラインからの距離（縦方向）
- $y$: リンク中央からの距離（横方向）
- $f$: 求めたい「セーブ確率の曲面」を表す関数
- 
私たちの目的は、この未知の関数 $f$ の形をデータから浮かび上がらせることです。

## 3.2 距離：ピタゴラスの定理の応用
空間相関を計算するためには、「2つのシュート地点がどれくらい離れているか」を測る定規が必要です。これは中学で習う**三平方の定理（ピタゴラスの定理）**そのものです。
地点A $(x_1, y_1)$ と 地点B $(x_2, y_2)$ の間の直線距離 $d$ は以下の式で表されます。

$$d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$$

この距離 $d$ が小さいほど、2つの地点でのセーブ確率は強く関連し（相関が高くなり）、$d$ が大きいほど無関係に近づきます。

## 3.3 ガウス過程（Gaussian Process）の直感
「関数 $f$ の形がわからない」という問題に対処するために、 **ガウス過程（GP）** を用います。

#### 直感的なアナロジー：
ゴムシートを想像してください。このシートは空中に浮かんでおり、最初は平らです（これが「事前分布」です）。

- 観測データとして「セーブ」があった場所では、シートを下から押し上げます。
- 「ゴール」があった場所では、シートを上から押し下げます。
- シートはゴム製なので、ある点を押すと、その周辺も一緒に動きます。この「周辺がどれくらい一緒に動くか」を決めるのが、ゴムの硬さや張り具合です。

ガウス過程とは、この「データに合わせて形を変える柔軟なゴムシート」を数学的に表現したものです。数式上では、無限次元の多変量正規分布として定義されますが、実務上は「滑らかな曲面を推定する機械」と捉えて差し支えありません。

## 4. データセットの準備と前処理
それでは、実践に入ります。まずは分析環境を構築し、データを準備します。ここではNHL（北米アイスホッケーリーグ）の標準的なデータ形式を模したデータセットを生成して使用します。

### 4.1 ライブラリのインポート
PyMCによるベイズモデリングを中心に、データ操作にPandas、可視化にMatplotlibとSeabornを使用します。また、ホッケーリンクの描画には専用ライブラリ hockey-rink を使用すると便利ですが、今回は汎用性を重視し、Matplotlibでリンクの概形を描画するコードも含めます。

```Python


import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import expit  # シグモイド関数（ロジスティック関数の逆関数）

# 乱数シードの設定（再現性のため）
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# プロットのスタイル設定
az.style.use("arviz-darkgrid")
plt.rcParams['figure.figsize'] = 
```

### 4.2 データの構造と生成
NHLのプレイバイプレイデータ（Play-by-Play Data）には通常、以下の情報が含まれています。
- X座標: リンクの長手方向（通常 -100フィートから +100フィート）
- Y座標: リンクの短手方向（通常 -42.5フィートから +42.5フィート）
- イベントタイプ: SHOT（セーブされたシュート）、GOAL（ゴール）

ここでは、学習用に合成データを生成します。現実の物理法則を模倣し、「ゴールに近いほど、またゴール正面に近いほどセーブ確率が下がる」という構造を埋め込みます。

```Python


def generate_synthetic_data(n_shots=2000):
    """
    ホッケーのシュートデータをシミュレートする関数
    """
    # X座標: 攻撃ゾーン（ブルーライン付近からゴールまで）
    # NHLのオフェンスゾーンは大体 x > 25 のエリア
    x = np.random.uniform(25, 95, n_shots)
    
    # Y座標: リンクの幅いっぱい
    y = np.random.uniform(-40, 40, n_shots)
    
    # ゴール位置（NHLでは x=89, y=0 付近）
    goal_x, goal_y = 89, 0
    
    # ゴールまでの距離を計算（三平方の定理）
    dist_to_goal = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
    
    # ゴールへの角度（ラジアン）を計算（三角関数）
    # ゴール正面（y=0）からのずれを表す
    # arctan2(y, x_dist) で計算可能
    angle_to_goal = np.arctan2(y, goal_x - x)
    
    # 真のセーブ確率を定義（これは神のみぞ知る関数。モデルでこれを推定したい）
    # 距離が近いほど確率は下がる（ロジスティック曲線を利用）
    # 角度がつくと（サイドに行くと）確率は少し下がるという仮定を入れる
    base_logit = (dist_to_goal - 20) / 10  # 距離依存項
    angle_penalty = -2.0 * np.abs(np.sin(angle_to_goal)) # 角度ペナルティ
    
    true_logit = base_logit + angle_penalty
    
    # 確率に変換（シグモイド関数：数値を0~1に押し込める）
    # p = 1 / (1 + e^(-x))
    true_prob = 1 / (1 + np.exp(-true_logit))
    
    # ベルヌーイ試行による結果の生成（0: ゴール, 1: セーブ）
    outcomes = np.random.binomial(1, true_prob)
    
    df = pd.DataFrame({
        'x': x,
        'y': y,
        'is_save': outcomes,
        'true_prob': true_prob  # 検証用に保持
    })
    
    return df

# データ生成
df_shots = generate_synthetic_data(1500)

# データの確認
print(df_shots.head())
print(f"全体のセーブ率: {df_shots['is_save'].mean():.3f}")
```

### 4.3 データの標準化（DXコンサルタントの実務ポイント）
ベイズモデリング、特にMCMC（マルコフ連鎖モンテカルロ法）を用いる際、データのスケール（数値の大きさ）は計算速度と収束に致命的な影響を与えます。ホッケーのリンクは $100 \times 85$ フィートですが、これをそのままモデルに入れると計算が不安定になります。データを平均0、標準偏差1程度に収める **標準化（Standardization）** は必須の前処理です。

```Python


# 座標データの標準化
# スケーリング係数を保存しておき、後で元の座標に戻せるようにする
x_mean, x_std = df_shots['x'].mean(), df_shots['x'].std()
y_mean, y_std = df_shots['y'].mean(), df_shots['y'].std()

df_shots['x_scaled'] = (df_shots['x'] - x_mean) / x_std
df_shots['y_scaled'] = (df_shots['y'] - y_mean) / y_std

# モデル入力用の行列を作成
X = df_shots[['x_scaled', 'y_scaled']].values
y = df_shots['is_save'].values
```


## 5. 理論的背景：ガウス過程と計算量の壁
ここで、少し専門的な話に踏み込みます。しかし、数式の意味を平易な言葉で解きほぐします。

### 5.1 ガウス過程のカーネル（共分散関数）
ガウス過程の中核には、カーネル関数 $k(x_i, x_j)$ が存在します。これは「地点 $i$ と地点 $j$ の相関の強さ」を定義する数式です。
最も一般的に使われる指数二次カーネル（Exponentiated Quadratic Kernel）、別名RBFカーネルの式を見てみましょう。

$$k(x_i, x_j) = \eta^2 \exp \left( - \frac{ ||x_i - x_j||^2 }{ 2\ell^2 } \right)$$

#### 数式の解剖（中学数学レベル）：
1. $||x_i - x_j||^2$: 2点間の距離の2乗です。
2. $\exp(- \dots)$: 指数関数です。中身がマイナスなので、距離が大きくなるほど、値は急速に0に近づきます。「遠く離れると関係なくなる」ことを表します。
3. $\ell$ (Length Scale, 特性長): 分母にあるこのパラメータが重要です。
    - $\ell$ が大きいと、分母が大きくなり、指数の中身が小さくなります。つまり、遠く離れても相関が残ります（＝滑らかでゆったりした変化）。
    - $\ell$ が小さいと、少し離れただけで相関が消えます（＝急激に変化する凸凹した表面）。
4. $\eta$ (Amplitude, 振幅): 関数の縦方向の変動幅を決めます。

### 5.2 $O(N^3)$ の壁とヒルベルト空間近似（HSGP）
ガウス過程は強力ですが、致命的な弱点があります。それは計算コストです。
$N$ 個のデータがある場合、$N \times N$ の相関行列を作り、その逆行列などを計算する必要があります。この計算量は $O(N^3)$、つまりデータ数が2倍になると計算時間は8倍、10倍になると1000倍になります。
数千件のシュートデータならまだしも、シーズン全体の数万件のデータを扱うと、通常のPCでは計算が終わりません。

そこで登場するのが、**ヒルベルト空間ガウス過程（Hilbert Space Gaussian Process, HSGP）** です。これはPyMC Labsが推奨する最新の手法です。

#### HSGPの直感（音楽の授業のアナロジー）：
複雑な音波（関数）を記録しようとするとき、すべての瞬間の波の高さを記録する（これが通常のGP）代わりに、「ドの音」「レの音」「ミの音」といった基本的な周波数の波（正弦波）をどれくらいの強さで混ぜ
合わせるかで表現する（フーリエ級数展開）ことができます。

HSGPは、空間上の複雑なセーブ確率の曲面を、あらかじめ決められたいくつかの「基本的な波（基底関数）」の足し合わせで近似します。


$$f(x) \approx \sum_{j=1}^{m} \beta_j \phi_j(x)$$

- $\phi_j(x)$: 基本的な波（正弦波など）。計算済みで固定。
- $\beta_j$: その波をどれくらい強く混ぜるかという係数。これをベイズ推定します。


これにより、計算量は $O(N \times m)$ （$m$ は基底関数の数）まで劇的に削減され、ビッグデータでも高速に解析できるようになります。

## 6. PyMCによる実装：HSGPモデルの構築
いよいよコードの実装です。PyMCの pm.gp.HSGP クラスを使用します。

### 6.1 モデルの定義

```Python


import pymc.gp.cov as cov

# 基底関数の数（m）と境界係数（c）の設定
# m: 波の数。多いほど精細な表現ができるが計算は重くなる。2次元なので[x方向, y方向]
# c: 境界係数。データの範囲よりも少し広い範囲で波を定義する必要がある。通常1.2〜2.0程度
m_list =   
c_val = 1.5

with pm.Model() as model_hsgp:
    # --- 1. ハイパーパラメータの事前分布（Priors） ---
    
    # Length Scale (ell): 相関が及ぶ距離
    # データのスケールが標準化されているため、1.0付近の値が妥当と想定
    # InverseGamma（逆ガンマ分布）は、0になるのを防ぐ（0になると無限に振動してしまうため）
    ell = pm.InverseGamma("ell", alpha=4, beta=1, shape=2) 
    
    # Amplitude (eta): 変動の大きさ
    # Exponential（指数分布）で正の値を制約
    eta = pm.Exponential("eta", lam=1.0)
    
    # --- 2. カーネル関数の定義 ---
    # Matern52カーネルを使用。RBFよりも現実の物理現象（滑らかすぎない）に近いとされる
    cov_func = eta**2 * pm.gp.cov.Matern52(2, ls=ell)
    
    # --- 3. HSGPの実装 ---
    # ここで魔法のような近似が行われる
    # Xは観測地点の座標行列
    gp = pm.gp.HSGP(m=m_list, c=c_val, cov_func=cov_func)
    
    # 潜在関数 f の推定
    # fは「対数オッズ（Log-Odds）」を表す実数値
    f = gp.prior("f", X=X)
    
    # --- 4. リンク関数と尤度（Likelihood） ---
    # f は -∞ ~ +∞ の値をとるため、確率 0 ~ 1 に変換する必要がある
    # ロジスティック関数（逆ロジット変換）を使用
    p_save = pm.Deterministic("p_save", pm.math.invlogit(f))
    
    # 観測データとの照合
    # ベルヌーイ分布：確率pで1（セーブ）、1-pで0（ゴール）が出る分布
    obs = pm.Bernoulli("obs", p=p_save, observed=y)
```

#### コード解説（注釈）：
- pm.Model(): PyMCのコンテキストマネージャ。このブロック内に書かれた変数はすべて1つの統計モデルの一部として管理されます。
- pm.InverseGamma: なぜ逆ガンマ分布なのか？ $\ell$ が0に近づくと、カーネル関数は無限に鋭くなり、データ点の一つ一つに過剰適合（Overfitting）してしまいます。これを防ぐために、0付近の確率が低い分布を選びます。
- pm.gp.HSGP: これが本レポートの主役です。従来の pm.gp.Latent の代わりにこれを使うだけで、裏側でヒルベルト空間近似が走ります。
- pm.Deterministic: モデルの内部変数を保存するためのラッパーです。事後分析で p_save の値を確認したいために使用しています。

### 6.2 サンプリング（推論の実行）
モデルを定義したら、MCMCを実行してパラメータの事後分布を求めます。

```Python


with model_hsgp:
    # NUTS（No-U-Turn Sampler）という高性能なアルゴリズムが自動選択される
    # tune: ウォームアップ期間。サンプラーが適切な歩幅を見つけるための試行錯誤
    # draws: 実際のサンプリング数
    # chains: 並列実行数。結果の安定性を確認するために複数走らせる
    idata = pm.sample(draws=1000, tune=1000, chains=2, target_accept=0.9, random_seed=RANDOM_SEED)
```

#### 実務上の注意点：
target_accept はデフォルトで0.8ですが、空間モデルは複雑なため、サンプリングが失敗（Divergence）しやすい傾向があります。これを0.9や0.95に上げることで、サンプラーの歩幅を小さくし、より慎重に探索させることでエラーを回避できます。

## 7. モデルの診断と検証
サンプリングが終わったら、結果が信頼できるかを確認します。

### 7.1 トレースプロットの確認

```Python


# トレースプロットの表示
# 左側：分布の形。きれいな山型ならOK。
# 右側：サンプリングの軌跡。毛虫のようにランダムに上下していればOK。
az.plot_trace(idata, var_names=["ell", "eta"]);
plt.show()
```


### 7.2 収束診断（R-hat）
$\hat{R}$（R-hat）は、複数のチェーン（並列実行したサンプリング）が同じ分布に収束したかを示す指標です。

判定基準：
- 1.01未満なら良好。
- 1.05を超えると収束していない可能性が高い。

```Python


summary = az.summary(idata, var_names=["ell", "eta"])
print(summary)
```


## 8. 予測と可視化：セーブ確率マップの作成
モデルが学習した内容を可視化します。リンク上のあらゆる地点 $(x, y)$ について、モデルが予測するセーブ確率を計算し、ヒートマップとして描画します。
### 8.1 グリッドデータの作成

```Python


# リンク全体を網羅するグリッドポイントを作成
x_grid = np.linspace(25, 100, 50)  # 攻撃ゾーン
y_grid = np.linspace(-42.5, 42.5, 50)
X_mesh, Y_mesh = np.meshgrid(x_grid, y_grid)

# グリッド座標の配列化
X_new = np.vstack().T

# 重要：学習時と同じスケーリングを適用する
X_new_scaled = np.zeros_like(X_new)
X_new_scaled[:, 0] = (X_new[:, 0] - x_mean) / x_std
X_new_scaled[:, 1] = (X_new[:, 1] - y_mean) / y_std
```

### 8.2 条件付き予測（Conditional Prediction）
学習済みモデルに新しい座標 X_new_scaled を入力し、予測値を生成します。HSGPの利点は、この予測計算も非常に高速であることです。

```Python


with model_hsgp:
    # 条件付き分布の作成（新しい点での f の値を予測）
    f_pred = gp.conditional("f_pred", Xnew=X_new_scaled)
    
    # 事後予測サンプリング
    # idata（学習結果）を使って、f_pred の分布を生成
    ppc = pm.sample_posterior_predictive(idata, var_names=["f_pred"])

# 予測結果の取り出し
# (chain, draw, grid_points) の次元になっているので平均をとる
f_pred_mean = ppc.posterior_predictive["f_pred"].mean(dim=["chain", "draw"]).values
# 確率に変換
p_pred_mean = expit(f_pred_mean).reshape(50, 50)
```

### 8.3 リンク上へのプロット
ここでは matplotlib を使って簡易的なリンクを描画し、その上に確率を重ねます。

```Python


def draw_rink_simple(ax):
    """簡易的なホッケーリンクの描画"""
    # リンクの枠
    ax.add_patch(plt.Rectangle((25, -42.5), 75, 85, fill=False, color="black", lw=2))
    # ゴールライン
    ax.axvline(89, color="red", lw=2, alpha=0.5)
    # ゴールクリース（半円）
    crease = plt.Circle((89, 0), 6, color="lightblue", alpha=0.5, zorder=0)
    ax.add_patch(crease)
    # ゴール位置
    ax.plot(89, 0, 'ks', markersize=5)

fig, ax = plt.subplots(figsize=(10, 8))

# コンタープロット（等高線）の描画
# cmap='RdYlBu'：赤が低い確率（危険）、青が高い確率（安全）
contour = ax.contourf(X_mesh, Y_mesh, p_pred_mean, levels=20, cmap="RdYlBu", alpha=0.8)

# リンクの枠線を描画
draw_rink_simple(ax)

# カラーバーとラベル
cbar = plt.colorbar(contour, ax=ax)
cbar.set_label("Estimated Save Probability")
ax.set_title("Goalie Save Probability Surface (HSGP Model)")
ax.set_xlabel("Distance from Center Line (ft)")
ax.set_ylabel("Distance from Center Ice (ft)")
ax.set_aspect('equal')

plt.show()
```

## 9. インサイトの抽出とビジネス応用
このヒートマップから何が読み取れるでしょうか？そして、それはビジネス（DXコンサルティング）にどう応用できるでしょうか？

### 9.1 データからの発見（2次・3次の洞察）
1. 「ロイヤルロード」の視覚化
    - モデルの結果を見ると、ゴール正面（スロット）のセーブ確率が最も低く、角度がつくと確率は上がることがわかります。しかし、さらに興味深いのは、特定のゴールキーパーにおいて「左側からのシュートに弱い」といった非対称性が発見される場合です。これは、ブロッカー（棒を持つ手）側とキャッチング（グローブを持つ手）側の技術差を示唆する可能性があります。
2. 不確実性の定量化
    - 通常の機械学習（勾配ブースティングなど）では点推定値しか出ませんが、ベイズモデルでは「予測の分散（標準偏差）」も計算できます。データが少ないエリアでは分散が大きくなります。
    - インサイト: 「このエリアからのシュートは苦手だ」と断定する前に、「このエリアはデータ不足で評価できない（分散が大きい）」という判断ができることは、誤った意思決定（選手の解雇や戦術変更）を防ぐ上で極めて重要です。

### 9.2 ビジネス/DXへの応用
この「空間ベイズモデリング」の考え方は、スポーツ以外にも直ちに応用可能です。

ホッケーの事例|ビジネス応用事例|共通する数学的構造
---|---|---
ゴールキーパーのセーブ率|店舗の来店コンバージョン率|2次元平面上の確率分布
リンク上の座標 $(x, y)$ | 商圏マップ上の座標（緯度・経度）|空間相関を持つ説明変数
シュートの難易度補正|競合店との距離による補正|共変量による補正項
データ不足エリアの推論|新規出店候補地の売上予測|空間補間（Spatial Interpolation）

DX推進コンサルタントとして、クライアントのPOSデータやGPSデータに対してこのHSGPモデルを適用すれば、「店舗Aは売上が低いが、立地の悪さを考慮すれば（空間的ハンデを補正すれば）、店長のマネジメント能力は実は高い」といった、表面的な数字の裏にある真実をあぶり出すことができます。

### 9.3 結論
本レポートでは、PyMCとHSGPを用いて、アイスホッケーのゴールキーパー評価モデルをゼロから構築しました。
重要なポイントは以下の3点です。
1. コンテキストの数式化: 「空間」というコンテキストをガウス過程という数学的ツールで表現した。
2. 計算の壁の突破: ヒルベルト空間近似を用いることで、実務で使える計算速度を実現した。
3. 不確実性への誠実さ: ベイズアプローチにより、データが少ないことによるリスクを定量化した。

単なる「平均値」の管理から脱却し、背後にある構造（関数）を理解しようとするこのアプローチこそが、データドリブンな意思決定の本質です。

---

#### 付録：数学用語の解説（中学生向け注釈）
- 関数 $f(x)$: 入力 $x$ を入れると、決まったルールで変換して出力 $y$ を出す箱のようなもの。ここでは「場所」を入れると「セーブのしやすさ」を出力する箱。
- 確率分布: サイコロの目が出る確率のように、値がどのようにばらつくかを表した図。ベイズ統計では、答えを「ひとつの値」ではなく「ありそうな値の山（分布）」として考える。
- ロジット（対数オッズ）: 確率 $p$ (0から1) を $-\infty$ から $+\infty$ の範囲に広げた値。計算しやすいのでモデルの内部ではこれを使い、最後に確率に戻す。
- シグモイド関数: すべり台のような形の関数。どんな大きな数字も小さな数字も、なめらかに0と1の間に押し込む役割を持つ。

#### 引用・参考文献
本レポートの作成にあたり、以下の技術的リソースおよび概念を参照しました。
NHLデータの構造と座標系: 1
ガウス過程とHSGPのPyMC実装詳細: 3
ガウス過程の直感的理解: 6
ホッケーリンクの可視化手法: 8
引用文献
- [National Hockey League Shots - Kaggle](https://www.kaggle.com/datasets/mexwell/national-hockey-league-shots), 12月 18, 2025にアクセス、 
- [NHL-Shot-Maps - Kaggle](https://www.kaggle.com/code/elorabrenneman/nhl-shot-maps), 12月 18, 2025にアクセス、 
- [pymc.gp.HSGP — PyMC dev documentation](https://www.pymc.io/projects/docs/en/latest/api/gp/generated/pymc.gp.HSGP.html), 12月 18, 2025にアクセス、 
- [Gaussian Processes: HSGP Reference & First Steps — PyMC example gallery](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/HSGP-Basic.html), 12月 18, 2025にアクセス、 
- [Gaussian Processes: HSGP Advanced Usage — PyMC example gallery](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/HSGP-), 12月 18, 2025にアクセス、 Advanced.html
- [Intuitive Intro To Gaussian Processes | by Omar Reid | Analytics Vidhya - Medium](https://medium.com/analytics-vidhya/intuitive-intro-to-gaussian-processes-328740cdc37f
), 12月 18, 2025にアクセス、
- [Gaussian Processes, not quite for dummies - The Gradient](https://thegradient.pub/gaussian-process-not-quite-for-dummies/), 12月 18, 2025にアクセス、 
- [hockey-rink - PyPI](https://pypi.org/project/hockey-rink/0.1.2/), 12月 18, 2025にアクセス、 
